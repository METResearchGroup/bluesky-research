v1_prompt = ""

batched_prompt = ""

# maybe something about how the LLM will get an enumerated list of texts,
# and it in turn has to return a JSON with the same number of items?
# I'll have to see how this looks.
