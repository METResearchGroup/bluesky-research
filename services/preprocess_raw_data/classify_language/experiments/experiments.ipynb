{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying language\n",
    "\n",
    "We want to classify the language of our posts. Since this is one of our filtering steps and we want it to act quickly, we'll benchmark a few options and check for (1) runtime and (2) memory usage. We want to measure runtime and memory usage after the initial cold start, since downloading, loading, and preparing the model can take some time.\n",
    "\n",
    "I assume that in terms of accuracy, the more up-to-date the model is, the better it will perform, but that all industrial-grade solutions are probably equally accurate. They might have differences in terms of compute and runtime, but besides light QA I won't check the accuracy that much, unless we see a drastic difference in classifications between models. Language classification should be pretty well-solved and I'm satisfied with using off-the-shelf solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_tooling.inference_helpers import classify_posts, generate_batches_of_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "For this, let's benchmark by loading 50,000 posts. Let's see how well our models do on that load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.sync.stream.helper import get_posts_as_list_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posts = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts: list[dict] = get_posts_as_list_dicts(k=num_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess the posts first. Let's manage any spacing or newline characters, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_post(post: dict) -> dict:\n",
    "    \"\"\"Do any preprocessing needed for the posts.\n",
    "\n",
    "    We only need the following fields:\n",
    "    - id\n",
    "    - uri\n",
    "    - text\n",
    "\n",
    "    We also want to remove any weird spacing or any newline characters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"uri\": post[\"uri\"],\n",
    "        \"text\": post[\"text\"].replace(\"\\n\", \" \").strip(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_posts: list[dict] = [preprocess_post(post) for post in posts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get our batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches: list[list[dict]] = generate_batches_of_posts(\n",
    "    posts=preprocessed_posts, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detectors\n",
    "In Python, there are plenty of tools to use for language detection. We'll try several of these options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use `langdetect`? [langdetect](https://github.com/Mimino666/langdetect) is a Python package (ported from [Java](https://www.slideshare.net/shuyo/language-detection-library-for-java)). It powers `spacy-langdetect` and is also commonly used in language detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.detector import LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"This is an example post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_english_langdetect(text):\n",
    "    return detect(text) == \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_post_langdetect(post: dict) -> dict:\n",
    "    \"\"\"Classify if a post is in English using the langdetect library.\"\"\"\n",
    "    try:\n",
    "        label = text_is_english_langdetect(post[\"text\"])\n",
    "    except LangDetectException as e:\n",
    "        # if unable to detect language, classify as False by default.\n",
    "        label = False\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"uri\": post[\"uri\"],\n",
    "        \"text\": post[\"text\"],\n",
    "        \"is_english_label\": label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for classify_posts: 0 minutes, 58 seconds\n",
      "Memory usage for classify_posts: 79.4375 MB\n"
     ]
    }
   ],
   "source": [
    "langdetect_labels: list[dict] = classify_posts(\n",
    "    posts=preprocessed_posts, clf_func=clf_post_langdetect,\n",
    "    batch_size=BATCH_SIZE, rate_limit_per_minute=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`langdetect` was really inefficient - it took 58 seconds and used ~80MB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Langid\n",
    "\n",
    "`langid` is a Python package designed specifically for language detection. According to the [docs](https://github.com/saffsd/langid.py), it's supposed to be fast, minimalistic, pre-trained, and not sensitive to domain-specific features (like markup text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_english_langid(text):\n",
    "    return langid.classify(text)[0] == \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_is_english_langid(\"This is a post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_post_langid(post: dict) -> dict:\n",
    "    \"\"\"Classify if a post is in English using the langid library.\"\"\"\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"uri\": post[\"uri\"],\n",
    "        \"text\": post[\"text\"],\n",
    "        \"is_english_label\": text_is_english_langid(post[\"text\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for classify_posts: 2 minutes, 36 seconds\n",
      "Memory usage for classify_posts: 53.046875 MB\n"
     ]
    }
   ],
   "source": [
    "langid_labels: list[dict] = classify_posts(\n",
    "    posts=preprocessed_posts, clf_func=clf_post_langid,\n",
    "    batch_size=BATCH_SIZE, rate_limit_per_minute=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 minutes and 36 seconds (156 seconds) to classify 50,000 posts. Used >50MB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fasttext\n",
    "\n",
    "`fasttext` is a [package](https://github.com/facebookresearch/fastText) developed at Facebook for fast, scalable word representation and language learning. They have a specific fine-tuned version, [fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) used for language detection.\n",
    "\n",
    "There are two ways to use `fasttext`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Hugging Face\n",
    "We can download the model from the Hugging Face Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model: fasttext.FastText._FastText = fasttext.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__eng_Latn',), array([1.00001001]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"This is a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_english_hf_fasttext(text):\n",
    "    return model.predict(text)[0][0] == \"__label__eng_Latn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_post_hf_fasttext(post: dict) -> dict:\n",
    "    \"\"\"Classify if a post is in English using the fasttext model from\n",
    "    Hugging Face Hub.\"\"\"\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"uri\": post[\"uri\"],\n",
    "        \"text\": post[\"text\"],\n",
    "        \"is_english_label\": text_is_english_hf_fasttext(post[\"text\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for classify_posts: 0 minutes, 4 seconds\n",
      "Memory usage for classify_posts: 8.4375 MB\n"
     ]
    }
   ],
   "source": [
    "hf_fasttext_labels: list[dict] = classify_posts(\n",
    "    posts=preprocessed_posts, clf_func=clf_post_hf_fasttext,\n",
    "    batch_size=BATCH_SIZE, rate_limit_per_minute=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 seconds to classify 50,000 posts. Used ~5 MB. Let's take a look at some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_english_list_hf_fasttext = [\n",
    "    post[\"is_english_label\"] for post in hf_fasttext_labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of English posts: 11583\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of English posts: {sum(is_english_list_hf_fasttext)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Local binary\n",
    "We can download the binary classifier model [here](https://fasttext.cc/docs/en/language-identification.html) and load it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# need to download the model; this is >100MB which is OK for local storage\n",
    "# but too large for Github (unless we use Github LFS).\n",
    "fasttext_model_bin = fasttext.load_model('lid.176.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_english_local_fasttext(text):\n",
    "    return fasttext_model_bin.predict(text)[0][0] == \"__label__eng_Latn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_post_local_fasttext(post: dict) -> dict:\n",
    "    \"\"\"Classify if a post is in English using a local binary of the fasttext\n",
    "    model.\"\"\"\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"uri\": post[\"uri\"],\n",
    "        \"text\": post[\"text\"],\n",
    "        \"is_english_label\": text_is_english_local_fasttext(post[\"text\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for classify_posts: 0 minutes, 1 seconds\n",
      "Memory usage for classify_posts: 4.015625 MB\n"
     ]
    }
   ],
   "source": [
    "local_fasttext_labels: list[dict] = classify_posts(\n",
    "    posts=preprocessed_posts, clf_func=clf_post_local_fasttext,\n",
    "    batch_size=BATCH_SIZE, rate_limit_per_minute=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 seconds to classify 50,000 posts. Used ~7 MB. Let's take a look at some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_english_list_local_fasttext = [\n",
    "    post[\"is_english_label\"] for post in local_fasttext_labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of English posts: 11583\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of English posts: {sum(is_english_list_local_fasttext)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the two `fasttext` models\n",
    "\n",
    "It looks like Hugging Face just stores the same model binary that we can store ourselves. We can store the model binary ourselves or use Hugging Face. I'd rather store it, especially since it's a pretty small binary (~120 MB), all things considered.\n",
    "\n",
    "We might want to run this multiple times but all else equal, having the binary version-controlled ourselves is better, so we don't have to rely on the network connection to Hugging Face.\n",
    "\n",
    "One developer's benchmark [tests](https://github.com/zafercavdar/fasttext-langdetect) seem to suggest that `fasttext` will work the fastest, and this does reinforce what I've found so far about `fasttext`. From the [Github repo](https://github.com/facebookresearch/fastText), we see that `fasttext` is quite fast and also widely uses. Conveniently, it's also optimized to work on CPU, as per the [FAQs](https://fasttext.cc/docs/en/faqs.html).\n",
    "\n",
    "I expect that the `fasttext` models will be the most performant, and out of those I would rather store the model binary than use the Hugging Face stored version of the model binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. spaCy\n",
    "\n",
    "Can we use `spaCy` for this task? Not really, as it turns out. [Here](https://github.com/explosion/spaCy/issues/11038) is a discussion about it in the Github repo. There's a third-party package, `spacy-langdetect`, but that uses `langdetect` under the hood. Examples such as [this](https://towardsdatascience.com/4-python-libraries-to-detect-english-and-non-english-language-c82ad3efd430) are outdated and don't work anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From these experiments, it seems clear that the fastest way to do language detectiopn is via `fasttext`. Without any parallelization or other speedups, we can classify ~50,000 posts in ~4 seconds, which is over an order of magnitude faster and more memory efficient than either `langdetect` or `langid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bluesky-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
