# Toxicity vs Join Date Analysis Pipeline

This directory contains the analysis pipeline for investigating the relationship between user join dates on Bluesky and the toxicity/outrage levels of their posts. The analysis uses Perspective API labels to measure toxicity and moral outrage probabilities, then correlates these with user join dates to understand how platform behavior evolves over time.

## Analysis Goal

The primary research question is: **Do users who joined Bluesky at different times exhibit different levels of toxicity and moral outrage in their posts?**

This analysis will help understand:
- Whether early adopters vs. later joiners have different communication patterns
- How platform culture may have evolved over time
- Potential correlations between join timing and content moderation needs

## Pipeline Overview

The analysis follows a six-stage pipeline:

1. **Daily Processing**: Calculate author-to-average toxicity/outrage scores for each study day
2. **Aggregation**: Combine daily results into weighted averages across the entire study period
3. **Visualization**: Analyze and visualize posting patterns to understand data distribution
4. **Sampling**: Extract a representative sample of top users for detailed analysis
5. **Profile Fetching**: Retrieve Bluesky profile data including join dates for correlation analysis
6. **Join Date Analysis**: Visualize toxicity/outrage patterns by user join date

## Files and Execution Order

### 1. `get_author_to_average_toxicity_outrage.py`
**Purpose**: Processes daily Perspective API labeled posts to calculate author-level toxicity and outrage averages for each partition date.

**What it does**:
- Loads Perspective API labels and preprocessed posts for each study day
- Joins the data to match posts with their toxicity/outrage scores
- Calculates weighted averages of `prob_toxic` and `prob_moral_outrage` per author per day
- Exports daily results to parquet files in the cache directory

**Key outputs**:
- Daily parquet files with columns: `author_did`, `preprocessing_timestamp`, `average_toxicity`, `average_outrage`, `total_labeled_posts`
- Each file represents one partition date's worth of author-level aggregations

**Execution**: Run first to generate daily data
```bash
python get_author_to_average_toxicity_outrage.py
```

### 2. `aggregate_author_to_average_toxicity_across_days.py`
**Purpose**: Aggregates the daily author-to-average toxicity/outrage data into comprehensive weighted averages across the entire study period.

**What it does**:
- Loads all daily parquet files generated by the first script
- Calculates weighted averages across all days for each author using the formula:
  ```
  weighted_avg = sum(posts_per_day * avg_per_day) / sum(posts_per_day)
  ```
- Sorts results by total posts (descending) to identify most active authors
- Exports final aggregated results to timestamped parquet files

**Key outputs**:
- Final parquet file with columns: `author_did`, `average_toxicity`, `average_outrage`, `total_labeled_posts`
- Results sorted by `total_labeled_posts` descending
- Exported to `results/<timestamp>/aggregated_author_toxicity_outrage.parquet`

**Execution**: Run second to aggregate daily data
```bash
python aggregate_author_to_average_toxicity_across_days.py
```

### 3. `visualize_number_posts_per_author.py`
**Purpose**: Creates visualizations to understand the distribution of posting behavior across authors in the aggregated dataset.

**What it does**:
- Loads the aggregated author-to-average toxicity/outrage data
- Removes outliers using the IQR method (statistically sound box-and-whisker plot definition)
- Creates a histogram showing the distribution of posts per author
- Adds vertical lines marking the top percentiles (0.1%, 1%, 5%, 10%) in different red hues
- Includes comprehensive statistics and insights about posting patterns

**Key outputs**:
- Histogram visualization saved as PNG in `visualizations/number_of_posts_per_author/<timestamp>/`
- Console output with detailed statistics about posting distribution
- Outlier removal statistics and percentile thresholds

**Execution**: Run third to analyze posting patterns
```bash
python visualize_number_posts_per_author.py
```

### 4. `sample_top_users.py`
**Purpose**: Samples the top 10% of users (by post count) for detailed analysis, particularly useful for join date retrieval and correlation analysis.

**What it does**:
- Loads the aggregated author-to-average toxicity/outrage data
- Removes outliers using the same IQR method as the visualization script
- Identifies the top 10% of users by post count (≥77 posts in current dataset)
- Randomly samples 1000 users from this top 10% group
- Saves the sampled data with user DIDs, toxicity/outrage scores, and post counts

**Key outputs**:
- Parquet file with columns: `author_did`, `average_toxicity`, `average_outrage`, `total_labeled_posts`
- Saved to `sampled_users/<timestamp>/top_10_percent_sampled_users.parquet`
- Console output with detailed statistics about the sampling process

**Execution**: Run fourth to prepare user sample for analysis
```bash
python sample_top_users.py
```

### 5. `get_bsky_profiles_for_sampled_users.py`
**Purpose**: Fetches Bluesky profile data for sampled users to obtain join dates and other profile information needed for correlation analysis.

**What it does**:
- Loads the sampled user data from the most recent sampling run
- Checks for existing profile data to avoid re-fetching
- Fetches Bluesky profiles via API using `get_author_record()` from `bluesky_helper.py`
- Processes users in configurable chunks (default: 100) with rate limiting
- Combines toxicity/outrage data with profile information including join dates
- Handles API errors gracefully (invalid DIDs, network issues, etc.)

**Key outputs**:
- Parquet files with columns: `author_did`, `average_toxicity`, `average_outrage`, `total_labeled_posts`, `created_at`, `description`, `display_name`, `followers_count`, `follows_count`, `handle`, `posts_count`
- Saved to `sampled_user_profiles/<timestamp>/user_profiles_chunk_XXX.parquet`
- Console output with detailed progress and summary statistics

**Execution**: Run fifth to fetch profile data for correlation analysis
```bash
python get_bsky_profiles_for_sampled_users.py
```

### 6. `visualize_toxicity_by_join_date.py`
**Purpose**: Analyzes and visualizes toxicity/outrage patterns by user join date to understand how platform behavior correlates with join timing.

**What it does**:
- Loads all profile data from `sampled_user_profiles/` across all timestamp directories
- Converts join dates to YYYY-MM format (groups pre-2024 dates as "2023-12")
- Calculates average toxicity and outrage scores for each join month
- Creates line graphs showing toxicity/outrage trends over time
- Adds vertical red markers for the study period (2024-10 to 2024-12)
- Generates separate and combined visualizations
- Creates user count histogram by join date

**Key outputs**:
- `toxicity_outrage_by_join_date.png` - Separate line plots for toxicity and outrage
- `combined_toxicity_outrage_by_join_date.png` - Combined line plot with dual y-axes
- `user_count_by_join_date.png` - Histogram of user counts by join month
- Saved to `visualizations/toxicity_by_join_date/<timestamp>/`
- Console output with detailed statistics and study period comparisons

**Execution**: Run sixth to analyze toxicity patterns by join date
```bash
python visualize_toxicity_by_join_date.py
```

### Debug Tools

#### `debug_user_join_counts.py`
**Purpose**: Debug visualization to show user join counts by month for troubleshooting data loading issues.

**What it does**:
- Loads all profile data from `sampled_user_profiles/` with detailed logging
- Creates histogram of user counts per join month
- Provides verbose output for debugging data loading across timestamp directories

**Key outputs**:
- Debug histogram saved to `visualizations/debug_join_counts/`
- Detailed console logs showing file loading progress

**Usage**: For troubleshooting data loading issues
```bash
python debug_user_join_counts.py
```

## Data Flow

```
Perspective API Labels + Preprocessed Posts
           ↓
    Daily Processing (get_author_to_average_toxicity_outrage.py)
           ↓
    Daily Author Aggregations (parquet files)
           ↓
    Cross-Day Aggregation (aggregate_author_to_average_toxicity_across_days.py)
           ↓
    Final Weighted Averages (results/<timestamp>/aggregated_author_toxicity_outrage.parquet)
           ↓
    Posting Pattern Analysis (visualize_number_posts_per_author.py)
           ↓
    Distribution Visualizations (visualizations/number_of_posts_per_author/<timestamp>/)
           ↓
    Top User Sampling (sample_top_users.py)
           ↓
    Sampled User Data (sampled_users/<timestamp>/top_10_percent_sampled_users.parquet)
           ↓
    Profile Data Fetching (get_bsky_profiles_for_sampled_users.py)
           ↓
    Combined Toxicity + Profile Data (sampled_user_profiles/<timestamp>/user_profiles_chunk_XXX.parquet)
           ↓
    Join Date Analysis (visualize_toxicity_by_join_date.py)
           ↓
    Toxicity by Join Date Visualizations (visualizations/toxicity_by_join_date/<timestamp>/)
```

## Study Parameters

- **Study Period**: Defined in `services/calculate_analytics/shared/constants.py`
- **Data Sources**: 
  - Perspective API labels (`ml_inference_perspective_api` service)
  - Preprocessed posts (`preprocessed_posts` service)
- **Processing Mode**: Sequential (to avoid memory issues)
- **Output Format**: Parquet files with timestamp-based organization

## Next Steps

After running both scripts, the aggregated data will be ready for:
1. **Join Date Retrieval**: Fetching user join dates via Bluesky API
2. **Correlation Analysis**: Analyzing relationships between join dates and toxicity/outrage
3. **Visualization**: Creating histograms and scatterplots to visualize patterns
4. **Statistical Analysis**: Determining significance of observed correlations

## Production Execution

Both scripts have corresponding SLURM submission scripts for production runs:
- `submit_toxicity_analysis_prod.sh` - For daily processing
- `submit_toxicity_aggregation_prod.sh` - For aggregation

These scripts handle resource allocation, environment setup, and logging for production-scale analysis.
