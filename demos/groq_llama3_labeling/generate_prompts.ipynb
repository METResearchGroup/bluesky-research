{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate prompts\n",
    "\n",
    "We have a separate notebook for generating the prompts that we want for our project. Let's create prompts that both have context and don't have context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from ml_tooling.llm.prompt_helper import generate_complete_prompt_for_post_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the list of links to get prompts for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"links_to_prompts_map.json\", 'r') as f:\n",
    "    links_to_prompt_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_list = [\n",
    "    link for (link, _) in links_to_prompt_map.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_prompts_lst = [\n",
    "    (link, prompt) for (link, prompt) in links_to_prompt_map.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot-checking to see if these are correct. If we see the same error\n",
    "# as before, the post in the link shouldn't match the prompt\n",
    "print(links_prompts_lst[200][0])\n",
    "print(links_prompts_lst[200][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot-checking to see if these are correct. If we see the same error\n",
    "# as before, the post in the link shouldn't match the prompt\n",
    "print(links_prompts_lst[300][0])\n",
    "print(links_prompts_lst[300][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot-checking to see if these are correct. If we see the same error\n",
    "# as before, the post in the link shouldn't match the prompt\n",
    "print(links_prompts_lst[-1][0])\n",
    "print(links_prompts_lst[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look good now, so let's dump these links so we can save those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links_list.json\", 'w') as f:\n",
    "    json.dump(links_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the prompts for each link. Our previous attempt to get the links assumes that we want context. Let's create new versions of the prompts, both with and without context, so we can test both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links_list.json\", 'r') as f:\n",
    "    loaded_links_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts_for_each_link(\n",
    "    links: list[str], task_name: str\n",
    ") -> dict:\n",
    "    \"\"\"Creates prompts for each link.\"\"\"\n",
    "    links_to_prompt_map = {}\n",
    "    for link in links:\n",
    "        try:\n",
    "            context_prompt = generate_complete_prompt_for_post_link(\n",
    "                link=link,\n",
    "                task_name=task_name,\n",
    "                include_context=True,\n",
    "                only_json_format=True\n",
    "            )\n",
    "            no_context_prompt = generate_complete_prompt_for_post_link(\n",
    "                link=link,\n",
    "                task_name=task_name,\n",
    "                include_context=False,\n",
    "                only_json_format=True\n",
    "            )\n",
    "            links_to_prompt_map[link] = {\n",
    "                \"context_prompt\": context_prompt,\n",
    "                \"no_context_prompt\": no_context_prompt,\n",
    "                # to see how often adding context actually changes our\n",
    "                # prompt so far.\n",
    "                \"prompts_are_equal\": context_prompt == no_context_prompt\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error with link {link}: {e}\")\n",
    "            continue\n",
    "    return links_to_prompt_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it on a subset, to make sure that we're on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_prompt_map = create_prompts_for_each_link(\n",
    "    loaded_links_list[0:10], \"civic_and_political_ideology\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_link = list(links_to_prompt_map.keys())[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://bsky.app/profile/jgownder.bsky.social/post/3knji5ltct32a'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompts = links_to_prompt_map[example_link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "In a drastic attempt to protect their beachfront homes, residents in Salisbury, Massachusetts, invested $500,000 in a sand dune to defend against encroaching tides. After being completed last week, the barrier made from 14,000 tons of sand lasted just 72 hours before it was completely washed away\n",
      "```\n",
      "\n",
      "\n",
      "The classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\n",
      "<Content referenced or linked to in the post>\n",
      " \n",
      "The post contains a external link to content with the following details:\n",
      "```\n",
      "[title]: $500K Dune Built to Protect Coastal Homes Lasts Just 3 Days\n",
      "[description]: The handmade barrier, which used 14,000 tons of sand, quickly crumbled.\n",
      "```\n",
      "\n",
      "\n",
      "```\n",
      "Again, the text of the post that needs to be classified is:\n",
      "```\n",
      "<text>\n",
      "In a drastic attempt to protect their beachfront homes, residents in Salisbury, Massachusetts, invested $500,000 in a sand dune to defend against encroaching tides. After being completed last week, the barrier made from 14,000 tons of sand lasted just 72 hours before it was completely washed away\n",
      "```\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n",
      "##########\n",
      "##########\n",
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "In a drastic attempt to protect their beachfront homes, residents in Salisbury, Massachusetts, invested $500,000 in a sand dune to defend against encroaching tides. After being completed last week, the barrier made from 14,000 tons of sand lasted just 72 hours before it was completely washed away\n",
      "```\n",
      "\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n",
      "##########\n",
      "##########\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(example_prompts[\"context_prompt\"])\n",
    "print('#' * 10)\n",
    "print('#' * 10)\n",
    "print(example_prompts[\"no_context_prompt\"])\n",
    "print('#' * 10)\n",
    "print('#' * 10)\n",
    "print(example_prompts[\"prompts_are_equal\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, these look great, so let me run these for all the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in ~8 minutes\n",
    "links_to_prompt_map: dict = create_prompts_for_each_link(\n",
    "    loaded_links_list, \"civic_and_political_ideology\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's spot-check these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_prompts_lst = [\n",
    "    (link, prompt) for (link, prompt) in links_to_prompt_map.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot-checking to see if these are correct. If we see the same error\n",
    "# as before, the post in the link shouldn't match the prompt\n",
    "print(links_prompts_lst[200][0])\n",
    "print(links_prompts_lst[200][1][\"context_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bsky.app/profile/paulgcornish.bsky.social/post/3knsux5iyet2h\n",
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "Looking forward to Kate being temporarily replaced by Cyborg Kate, Kate of Steel, Kate-boy, and the Last Daughter of Berkshire.\n",
      "```\n",
      "\n",
      "\n",
      "The classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\n",
      "<Content referenced or linked to in the post>\n",
      " \n",
      "The post contains the following alt text for its images:\n",
      "```\n",
      "Image 1 alt text: GB News tweet saying \"Princess Kate on track for Easter return.\"\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "```\n",
      "Again, the text of the post that needs to be classified is:\n",
      "```\n",
      "<text>\n",
      "Looking forward to Kate being temporarily replaced by Cyborg Kate, Kate of Steel, Kate-boy, and the Last Daughter of Berkshire.\n",
      "```\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spot-checking to see if these are correct. If we see the same error\n",
    "# as before, the post in the link shouldn't match the prompt\n",
    "print(links_prompts_lst[300][0])\n",
    "print(links_prompts_lst[300][1][\"context_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bsky.app/profile/merrittk.com/post/3kntzspsje52z\n",
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "its going to be so funny when trump dies as a result of falling in the shower or choking on a piece of steak\n",
      "```\n",
      "\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# spot-checking to see if these are correct. If we see the same error\n",
    "# as before, the post in the link shouldn't match the prompt\n",
    "print(links_prompts_lst[-1][0])\n",
    "print(links_prompts_lst[-1][1][\"context_prompt\"])\n",
    "print(links_prompts_lst[-1][1][\"prompts_are_equal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links_to_prompts_map_v2.json\", 'w') as f:\n",
    "    json.dump(links_to_prompt_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bluesky-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
