{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that we've labeled our data, let's check our results and make some conclusions from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_classifications = pd.read_csv(\n",
    "    \"../manuscript_pilot/representative_diversification_feed.csv\"\n",
    ")\n",
    "llama3_8b_classifications = pd.read_csv(\n",
    "    \"classified_posts_llama3_8b.csv\"\n",
    ")\n",
    "llama3_70b_classifications = pd.read_csv(\n",
    "    \"classified_posts_llama3_70b.csv\"\n",
    ")\n",
    "ground_truth_labels = pd.read_csv(\n",
    "    \"../manuscript_pilot/hand_labeled_pilot_posts.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and preprocessing\n",
    "Now let's do some cleaning and preprocessing so everything is in the same format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_subset = gpt4_classifications[[\"link\", \"civic\", \"political_ideology\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_subset[\"civic\"] = gpt_subset[\"civic\"].replace(True, \"civic\")\n",
    "gpt_subset[\"civic\"] = gpt_subset[\"civic\"].replace(False, \"not civic\")\n",
    "gpt_subset[\"political_ideology\"] = gpt_subset[\"political_ideology\"].replace(\n",
    "    \" left-leaning\", \"left-leaning\"\n",
    ")\n",
    "gpt_subset = gpt_subset.rename(\n",
    "    columns={\n",
    "        \"civic\": \"gpt4_civic_label\",\n",
    "        \"political_ideology\": \"gpt4_political_label\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'gpt4_civic_label', 'gpt4_political_label'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_subset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Llama models\n",
    "\n",
    "Let's only include the ones that have valid JSON responses, since these are the only ones that we could label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_subset = llama3_8b_classifications[\n",
    "    llama3_8b_classifications[\"valid_json_response\"] == True\n",
    "]\n",
    "llama3_8b_subset = llama3_8b_subset[\n",
    "    [\"link\", \"civic_label\", \"political_label\"]\n",
    "]\n",
    "llama3_8b_subset = llama3_8b_subset.rename(\n",
    "    columns={\n",
    "        \"civic_label\": \"llama3-8b_civic_label\",\n",
    "        \"political_label\": \"llama3-8b_political_label\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'llama3-8b_civic_label', 'llama3-8b_political_label'], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_8b_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_70b_subset = llama3_70b_classifications[\n",
    "    llama3_70b_classifications[\"valid_json_response\"] == True\n",
    "]\n",
    "llama3_70b_subset = llama3_70b_subset[\n",
    "    [\"link\", \"civic_label\", \"political_label\"]\n",
    "]\n",
    "llama3_70b_subset = llama3_70b_subset.rename(\n",
    "    columns={\n",
    "        \"civic_label\": \"llama3-70b_civic_label\",\n",
    "        \"political_label\": \"llama3-70b_political_label\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'llama3-70b_civic_label', 'llama3-70b_political_label'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_70b_subset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ground-truth labels\n",
    "\n",
    "Let's remove any NAs and then do processing like the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels = ground_truth_labels[\n",
    "    ~pd.isna(ground_truth_labels[\"civic_hand_label\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels_subset = ground_truth_labels[\n",
    "    [\"link\", \"civic_hand_label\", \"political_ideology_hand_label\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's get some basic counts and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civic\n",
      "not civic    187\n",
      "civic        174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(gpt_subset[\"gpt4_civic_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4_political_label\n",
      "left-leaning     140\n",
      "unclear           18\n",
      "right-leaning     15\n",
      "moderate           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    gpt_subset[\n",
    "        gpt_subset[\"gpt4_civic_label\"] == \"civic\"\n",
    "    ][\"gpt4_political_label\"].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-8b_civic_label\n",
      "civic        199\n",
      "not civic    148\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(llama3_8b_subset[\"llama3-8b_civic_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-8b_political_label\n",
      "left-leaning     130\n",
      "right-leaning     34\n",
      "unclear           32\n",
      "moderate           3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    llama3_8b_subset[\n",
    "        llama3_8b_subset[\"llama3-8b_civic_label\"] == \"civic\"\n",
    "    ][\"llama3-8b_political_label\"].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama3-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-70b_civic_label\n",
      "civic        200\n",
      "not civic    147\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(llama3_70b_subset[\"llama3-70b_civic_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-70b_political_label\n",
      "left-leaning     173\n",
      "right-leaning     11\n",
      "unclear            9\n",
      "moderate           7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    llama3_70b_subset[\n",
    "        llama3_70b_subset[\"llama3-70b_civic_label\"] == \"civic\"\n",
    "    ][\"llama3-70b_political_label\"].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civic_hand_label\n",
      "civic        193\n",
      "not civic    161\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_labels_subset[\"civic_hand_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political_ideology_hand_label\n",
      "left-leaning     160\n",
      "unclear           20\n",
      "right-leaning      7\n",
      "moderate           5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    ground_truth_labels_subset[\n",
    "        ground_truth_labels_subset[\"civic_hand_label\"] == \"civic\"\n",
    "    ][\"political_ideology_hand_label\"].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the data together\n",
    "\n",
    "Let's join the data together to get a joined version of the labels and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(\n",
    "    gpt_subset, llama3_8b_subset, on=\"link\"\n",
    ")\n",
    "joined_df = pd.merge(\n",
    "    joined_df, llama3_70b_subset, on=\"link\"\n",
    ")\n",
    "joined_df = pd.merge(\n",
    "    joined_df, ground_truth_labels_subset, on=\"link\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bluesky-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
