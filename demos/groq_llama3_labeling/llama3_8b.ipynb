{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling posts with Llama3-8b via Groq\n",
    "\n",
    "We'll use the LiteLLM [Groq](https://litellm.vercel.app/docs/providers/groq) connection to connect to Groq and use that to label via Llama3-8b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ml_tooling.llm.inference import run_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links_to_prompts_map_v2.json\") as f:\n",
    "    links_to_prompt_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_prompts_lst: list[tuple[str, dict]] = [\n",
    "    (link, prompt_dict)\n",
    "    for (link, prompt_dict) in links_to_prompt_map.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://bsky.app/profile/jbouie.bsky.social/post/3knqbtrdzrz2n',\n",
       " {'context_prompt': '\\n\\nPretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \\n\\nThen, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', or \\'unclear\\'. You are analyzing text that has been pre-identified as \\'political\\' in nature. If the text is not civic, return \"unclear\".\\n\\nThink through your response step by step.\\n\\nReturn in a JSON format in the following way:\\n{\\n    \"civic\": <two values, \\'civic\\' or \\'not civic\\'>,\\n    \"political_ideology\": <four values, \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', \\'unclear\\'>,\\n    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\\n    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\\n}\\n\\n\\nHere is the post text that needs to be classified:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\n\\nThe classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\\n<Content referenced or linked to in the post>\\n \\nThe post contains a external link to content with the following details:\\n```\\n[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\\n[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\\n```\\n\\n<URLs>\\n The post links to external URLs:\\nThis post links to a trustworthy news article.\\n\\n\\n```\\nAgain, the text of the post that needs to be classified is:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\nJustifications are not necessary.\\nReturn ONLY the JSON. I will parse the string result in JSON format.\\n',\n",
       "  'no_context_prompt': '\\n\\nPretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \\n\\nThen, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', or \\'unclear\\'. You are analyzing text that has been pre-identified as \\'political\\' in nature. If the text is not civic, return \"unclear\".\\n\\nThink through your response step by step.\\n\\nReturn in a JSON format in the following way:\\n{\\n    \"civic\": <two values, \\'civic\\' or \\'not civic\\'>,\\n    \"political_ideology\": <four values, \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', \\'unclear\\'>,\\n    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\\n    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\\n}\\n\\n\\nHere is the post text that needs to be classified:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\n\\nJustifications are not necessary.\\nReturn ONLY the JSON. I will parse the string result in JSON format.\\n',\n",
       "  'prompts_are_equal': False})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_prompts_lst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama3-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store all the. Llama3-8b results in one dictionary. Let's initialize that dictionary with the links and prompts from the input, and then for the cases where the prompt with context equals the prompt without context, we only run the query once in order to avoid duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_results: dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (link, prompt_dict) in links_to_prompt_map.items():\n",
    "    llama3_8b_results[link] = prompt_dict\n",
    "    llama3_8b_results[link][\"context_llm_result\"] = \"\"\n",
    "    llama3_8b_results[link][\"no_context_llm_result\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mapping of links to prompts for both context and without context prompts, so we can just iterate through those during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want to run the context classification for posts\n",
    "# that have context in the first place. If the prompts between\n",
    "# the context and no-context cases are the \n",
    "link_to_context_prompt_map: dict = {\n",
    "    link: prompt_dict[\"context_prompt\"]\n",
    "    for (link, prompt_dict) in llama3_8b_results.items()\n",
    "    if not prompt_dict[\"prompts_are_equal\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bsky.app/profile/jbouie.bsky.social/post/3knqbtrdzrz2n\n",
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "\n",
      "The classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\n",
      "<Content referenced or linked to in the post>\n",
      " \n",
      "The post contains a external link to content with the following details:\n",
      "```\n",
      "[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\n",
      "[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\n",
      "```\n",
      "\n",
      "<URLs>\n",
      " The post links to external URLs:\n",
      "This post links to a trustworthy news article.\n",
      "\n",
      "\n",
      "```\n",
      "Again, the text of the post that needs to be classified is:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spot-check and see if the prompt indeed has context.\n",
    "# It appears to be correct. These should all have prompts\n",
    "print(list(link_to_context_prompt_map.items())[0][0])\n",
    "print(list(link_to_context_prompt_map.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 204 posts with context\n",
    "len(link_to_context_prompt_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the prompts without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run the no-context prompts for all links\n",
    "link_to_no_context_prompt_map: dict = {\n",
    "    link: prompt_dict[\"no_context_prompt\"]\n",
    "    for (link, prompt_dict) in llama3_8b_results.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bsky.app/profile/jbouie.bsky.social/post/3knqbtrdzrz2n\n",
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "\n",
      "The classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\n",
      "<Content referenced or linked to in the post>\n",
      " \n",
      "The post contains a external link to content with the following details:\n",
      "```\n",
      "[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\n",
      "[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\n",
      "```\n",
      "\n",
      "<URLs>\n",
      " The post links to external URLs:\n",
      "This post links to a trustworthy news article.\n",
      "\n",
      "\n",
      "```\n",
      "Again, the text of the post that needs to be classified is:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spot-check and see if the prompt indeed has context.\n",
    "# It appears to be correct.\n",
    "print(list(link_to_context_prompt_map.items())[0][0])\n",
    "print(list(link_to_context_prompt_map.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 354 without context.\n",
    "len(link_to_no_context_prompt_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With context\n",
    "\n",
    "Let's run inference for llama3-8b with context and export the results. Let's then compare it to the ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_with_context_llm_results: list[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run queries against Llama3-8b model. This will take a while since we're\n",
    "# making a lot of requests to Groq. Takes ~30 minutes.\n",
    "for idx, prompt in enumerate(prompts_lst):\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processing post {idx + 1} of {total_prompts}\")\n",
    "    try:\n",
    "        result = run_query(prompt=prompt, model_name=\"Llama3-8b (via Groq)\")\n",
    "        llama3_8b_with_context_llm_results.append(result)\n",
    "    except Exception as e:\n",
    "        # for the ones that failed, this happens because we set a requirement\n",
    "        # that it must be valid JSON. Some of the responses are not JSON, so\n",
    "        # Groq throws an error and tells us that the result wasn't JSON.\n",
    "        print(f\"Error with post {post['link']}: {e}\")\n",
    "        llama3_8b_with_context_llm_results.append(\"\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bluesky-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
