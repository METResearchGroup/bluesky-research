{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling posts with Llama3-8b via Groq\n",
    "\n",
    "We'll use the LiteLLM [Groq](https://litellm.vercel.app/docs/providers/groq) connection to connect to Groq and use that to label via Llama3-8b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from ml_tooling.llm.inference import run_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links_to_prompts_map_v2.json\") as f:\n",
    "    links_to_prompt_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_prompts_lst: list[tuple[str, dict]] = [\n",
    "    (link, prompt_dict)\n",
    "    for (link, prompt_dict) in links_to_prompt_map.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://bsky.app/profile/jbouie.bsky.social/post/3knqbtrdzrz2n',\n",
       " {'context_prompt': '\\n\\nPretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \\n\\nThen, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', or \\'unclear\\'. You are analyzing text that has been pre-identified as \\'political\\' in nature. If the text is not civic, return \"unclear\".\\n\\nThink through your response step by step.\\n\\nReturn in a JSON format in the following way:\\n{\\n    \"civic\": <two values, \\'civic\\' or \\'not civic\\'>,\\n    \"political_ideology\": <four values, \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', \\'unclear\\'>,\\n    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\\n    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\\n}\\n\\n\\nHere is the post text that needs to be classified:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\n\\nThe classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\\n<Content referenced or linked to in the post>\\n \\nThe post contains a external link to content with the following details:\\n```\\n[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\\n[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\\n```\\n\\n<URLs>\\n The post links to external URLs:\\nThis post links to a trustworthy news article.\\n\\n\\n```\\nAgain, the text of the post that needs to be classified is:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\nJustifications are not necessary.\\nReturn ONLY the JSON. I will parse the string result in JSON format.\\n',\n",
       "  'no_context_prompt': '\\n\\nPretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \\n\\nThen, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', or \\'unclear\\'. You are analyzing text that has been pre-identified as \\'political\\' in nature. If the text is not civic, return \"unclear\".\\n\\nThink through your response step by step.\\n\\nReturn in a JSON format in the following way:\\n{\\n    \"civic\": <two values, \\'civic\\' or \\'not civic\\'>,\\n    \"political_ideology\": <four values, \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', \\'unclear\\'>,\\n    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\\n    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\\n}\\n\\n\\nHere is the post text that needs to be classified:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\n\\nJustifications are not necessary.\\nReturn ONLY the JSON. I will parse the string result in JSON format.\\n',\n",
       "  'prompts_are_equal': False})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_prompts_lst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama3-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store all the. Llama3-8b results in one dictionary. Let's initialize that dictionary with the links and prompts from the input, and then for the cases where the prompt with context equals the prompt without context, we only run the query once in order to avoid duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_results: dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (link, prompt_dict) in links_to_prompt_map.items():\n",
    "    llama3_8b_results[link] = prompt_dict\n",
    "    llama3_8b_results[link][\"context_llm_result\"] = \"\"\n",
    "    llama3_8b_results[link][\"no_context_llm_result\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mapping of links to prompts for both context and without context prompts, so we can just iterate through those during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want to run the context classification for posts\n",
    "# that have context in the first place. If the prompts between\n",
    "# the context and no-context cases are the \n",
    "link_to_context_prompt_map: dict = {\n",
    "    link: prompt_dict[\"context_prompt\"]\n",
    "    for (link, prompt_dict) in llama3_8b_results.items()\n",
    "    if not prompt_dict[\"prompts_are_equal\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bsky.app/profile/jbouie.bsky.social/post/3knqbtrdzrz2n\n",
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "\n",
      "The classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\n",
      "<Content referenced or linked to in the post>\n",
      " \n",
      "The post contains a external link to content with the following details:\n",
      "```\n",
      "[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\n",
      "[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\n",
      "```\n",
      "\n",
      "<URLs>\n",
      " The post links to external URLs:\n",
      "This post links to a trustworthy news article.\n",
      "\n",
      "\n",
      "```\n",
      "Again, the text of the post that needs to be classified is:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spot-check and see if the prompt indeed has context.\n",
    "# It appears to be correct. These should all have prompts\n",
    "print(list(link_to_context_prompt_map.items())[0][0])\n",
    "print(list(link_to_context_prompt_map.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 204 posts with context\n",
    "len(link_to_context_prompt_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the prompts without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run the no-context prompts for all links\n",
    "link_to_no_context_prompt_map: dict = {\n",
    "    link: prompt_dict[\"no_context_prompt\"]\n",
    "    for (link, prompt_dict) in llama3_8b_results.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bsky.app/profile/jbouie.bsky.social/post/3knqbtrdzrz2n\n",
      "\n",
      "\n",
      "Pretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \n",
      "\n",
      "Then, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are 'left-leaning', 'moderate', 'right-leaning', or 'unclear'. You are analyzing text that has been pre-identified as 'political' in nature. If the text is not civic, return \"unclear\".\n",
      "\n",
      "Think through your response step by step.\n",
      "\n",
      "Return in a JSON format in the following way:\n",
      "{\n",
      "    \"civic\": <two values, 'civic' or 'not civic'>,\n",
      "    \"political_ideology\": <four values, 'left-leaning', 'moderate', 'right-leaning', 'unclear'>,\n",
      "    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\n",
      "    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\n",
      "}\n",
      "\n",
      "\n",
      "Here is the post text that needs to be classified:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "\n",
      "The classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\n",
      "<Content referenced or linked to in the post>\n",
      " \n",
      "The post contains a external link to content with the following details:\n",
      "```\n",
      "[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\n",
      "[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\n",
      "```\n",
      "\n",
      "<URLs>\n",
      " The post links to external URLs:\n",
      "This post links to a trustworthy news article.\n",
      "\n",
      "\n",
      "```\n",
      "Again, the text of the post that needs to be classified is:\n",
      "```\n",
      "<text>\n",
      "that’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\n",
      "```\n",
      "\n",
      "Justifications are not necessary.\n",
      "Return ONLY the JSON. I will parse the string result in JSON format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spot-check and see if the prompt indeed has context.\n",
    "# It appears to be correct.\n",
    "print(list(link_to_context_prompt_map.items())[0][0])\n",
    "print(list(link_to_context_prompt_map.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 354 without context.\n",
    "len(link_to_no_context_prompt_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now export these prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"link_to_prompts_with_context.json\", 'w') as f:\n",
    "    json.dump(link_to_context_prompt_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"link_to_prompts_with_no_context.json\", 'w') as f:\n",
    "    json.dump(link_to_no_context_prompt_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With context\n",
    "\n",
    "Let's run inference for llama3-8b with context and export the results. Let's then compare it to the ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_with_context_llm_results: list[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_with_context_llm_errors: list[dict] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run queries against Llama3-8b model. This will take a while since we're\n",
    "# making a lot of requests to Groq. Takes ~10 minutes\n",
    "# (but up to 30 minutes, depending on rate limits)\n",
    "for idx, (link, prompt) in enumerate(link_to_context_prompt_map.items()):\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processing post {idx + 1} of {prompt}\")\n",
    "    try:\n",
    "        result = run_query(prompt=prompt, model_name=\"Llama3-8b (via Groq)\")\n",
    "        llama3_8b_with_context_llm_results.append(result)\n",
    "    except Exception as e:\n",
    "        # for the ones that failed, this happens because we set a requirement\n",
    "        # that it must be valid JSON. Some of the responses are not JSON, so\n",
    "        # Groq throws an error and tells us that the result wasn't JSON.\n",
    "        print(f\"Error with post {link}: {e}\")\n",
    "        llama3_8b_with_context_llm_errors.append(\n",
    "            {\n",
    "                \"idx\": idx,\n",
    "                \"link\": link,\n",
    "                \"prompt\": prompt,\n",
    "                \"error\": e\n",
    "            }\n",
    "        )\n",
    "        llama3_8b_with_context_llm_results.append(\"\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llama3_8b_with_context_llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llama3_8b_with_context_llm_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it couldn't classify 2 of the posts. These are the cases where it couldn't produce a valid JSON. We could easily just re-do these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': BadRequestError('GroqException - Error code: 400 - {\\'error\\': {\\'message\\': \"Failed to generate JSON. Please adjust your prompt. See \\'failed_generation\\' for more details.\", \\'type\\': \\'invalid_request_error\\', \\'code\\': \\'json_validate_failed\\', \\'failed_generation\\': \\'{\\\\n    \"civic\": \"civic\",\\\\n    \"political_ideology\": \"left-leaning\",\\\\n    \"reason_civic\": \"The post discusses a controversy surrounding the disposal of books featuring diverse characters and themes, highlighting issues of representation and censorship.\",\\\\n    \"reason_political_ideology\": \"The post\\\\\\'s focus on the importance of representation and the potential for censorship aligns with left-leaning values.\"\\'}}'),\n",
      " 'idx': 109,\n",
      " 'link': 'https://bsky.app/profile/clancyny.bsky.social/post/3knh2gspnxk2a',\n",
      " 'prompt': '\\n'\n",
      "           '\\n'\n",
      "           'Pretend that you are a classifier that predicts whether a post has '\n",
      "           'civic content or not. Civic refers to whether a given post is '\n",
      "           'related to politics (government, elections, politicians, activism, '\n",
      "           'etc.) or social issues (major issues that affect a large group of '\n",
      "           'people, such as the economy, inequality, racism, education, '\n",
      "           'immigration, human rights, the environment, etc.). We refer to any '\n",
      "           'content that is classified as being either of these two categories '\n",
      "           'as “civic”; otherwise they are not civic. Please classify the '\n",
      "           'following text denoted in <text> as \"civic\" or \"not civic\". \\n'\n",
      "           '\\n'\n",
      "           'Then, if the post is civic, classify the text based on the '\n",
      "           'political lean of the opinion or argument it presents. Your '\n",
      "           \"options are 'left-leaning', 'moderate', 'right-leaning', or \"\n",
      "           \"'unclear'. You are analyzing text that has been pre-identified as \"\n",
      "           \"'political' in nature. If the text is not civic, return \"\n",
      "           '\"unclear\".\\n'\n",
      "           '\\n'\n",
      "           'Think through your response step by step.\\n'\n",
      "           '\\n'\n",
      "           'Return in a JSON format in the following way:\\n'\n",
      "           '{\\n'\n",
      "           '    \"civic\": <two values, \\'civic\\' or \\'not civic\\'>,\\n'\n",
      "           '    \"political_ideology\": <four values, \\'left-leaning\\', '\n",
      "           \"'moderate', 'right-leaning', 'unclear'>,\\n\"\n",
      "           '    \"reason_civic\": <optional, a 1 sentence reason for why the '\n",
      "           'text is civic>,\\n'\n",
      "           '    \"reason_political_ideology\": <optional, a 1 sentence reason '\n",
      "           'for why the text has the given political ideology>\\n'\n",
      "           '}\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'Here is the post text that needs to be classified:\\n'\n",
      "           '```\\n'\n",
      "           '<text>\\n'\n",
      "           '\"Hundreds of new books featuring characters of color and LGBTQ+ '\n",
      "           'themes were found by the trash at a Staten Island elementary '\n",
      "           'school.\"\\n'\n",
      "           '\\n'\n",
      "           'In boxes designated “not approved,” books bore Post-It notes: '\n",
      "           '\"teenage girls having a crush on another girl in class\" and '\n",
      "           '“negative slant on white people.”\\n'\n",
      "           '```\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'The classification of a post might depend on contextual '\n",
      "           'information. For example, the text in a post might comment on an '\n",
      "           'image or on a retweeted post. Attend to the context where '\n",
      "           'appropriate. Here is some context on the post that needs '\n",
      "           'classification: ```\\n'\n",
      "           '<Content referenced or linked to in the post>\\n'\n",
      "           ' \\n'\n",
      "           'The post contains a external link to content with the following '\n",
      "           'details:\\n'\n",
      "           '```\\n'\n",
      "           '[title]: Books on Black history, immigration found in trash by '\n",
      "           'Staten Island school, sparking investigation\\n'\n",
      "           '[description]: Sticky notes on the books discarded from the '\n",
      "           'elementary school detailed apparent concerns about the contents.\\n'\n",
      "           '```\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           '```\\n'\n",
      "           'Again, the text of the post that needs to be classified is:\\n'\n",
      "           '```\\n'\n",
      "           '<text>\\n'\n",
      "           '\"Hundreds of new books featuring characters of color and LGBTQ+ '\n",
      "           'themes were found by the trash at a Staten Island elementary '\n",
      "           'school.\"\\n'\n",
      "           '\\n'\n",
      "           'In boxes designated “not approved,” books bore Post-It notes: '\n",
      "           '\"teenage girls having a crush on another girl in class\" and '\n",
      "           '“negative slant on white people.”\\n'\n",
      "           '```\\n'\n",
      "           '\\n'\n",
      "           'Justifications are not necessary.\\n'\n",
      "           'Return ONLY the JSON. I will parse the string result in JSON '\n",
      "           'format.\\n'}\n"
     ]
    }
   ],
   "source": [
    "pprint(llama3_8b_with_context_llm_errors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take the posts with context and their labels and export them. Let's only do the ones that have valid JSONs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_context_prompts = [\n",
    "    (link, prompt)\n",
    "    for (link, prompt) in link_to_context_prompt_map.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "# let's verify that # of links and prompts = # of results\n",
    "# n = 204 for both\n",
    "print(len(links_context_prompts))\n",
    "print(len(llama3_8b_with_context_llm_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now record the links, prompts, and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_with_links_and_context_prompts: dict[str, dict] = {\n",
    "    link: {\n",
    "        \"context_prompt\": prompt,\n",
    "        \"result\": result\n",
    "    }\n",
    "    for ((link, prompt), result)\n",
    "    in zip(\n",
    "        links_context_prompts, llama3_8b_with_context_llm_results\n",
    "    )\n",
    "    if result != \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_prompt': '\\n\\nPretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \\n\\nThen, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', or \\'unclear\\'. You are analyzing text that has been pre-identified as \\'political\\' in nature. If the text is not civic, return \"unclear\".\\n\\nThink through your response step by step.\\n\\nReturn in a JSON format in the following way:\\n{\\n    \"civic\": <two values, \\'civic\\' or \\'not civic\\'>,\\n    \"political_ideology\": <four values, \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', \\'unclear\\'>,\\n    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\\n    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\\n}\\n\\n\\nHere is the post text that needs to be classified:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\n\\nThe classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\\n<Content referenced or linked to in the post>\\n \\nThe post contains a external link to content with the following details:\\n```\\n[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\\n[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\\n```\\n\\n<URLs>\\n The post links to external URLs:\\nThis post links to a trustworthy news article.\\n\\n\\n```\\nAgain, the text of the post that needs to be classified is:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\nJustifications are not necessary.\\nReturn ONLY the JSON. I will parse the string result in JSON format.\\n', 'result': '{\\n    \"civic\": \"civic\",\\n    \"political_ideology\": \"left-leaning\",\\n    \"reason_civic\": \"The post references a specific policy proposal by Bernie Sanders, a left-leaning politician, and discusses the benefits of a four-day workweek.\",\\n    \"reason_political_ideology\": \"The post\\'s language and tone are consistent with left-leaning views on work-life balance and the role of government in promoting social welfare.\"\\n}'}\n"
     ]
    }
   ],
   "source": [
    "example_link = links_context_prompts[0][0]\n",
    "print(labels_with_links_and_context_prompts[example_link])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dump these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llama3_8b_context_prompts_results.json\", 'w') as f:\n",
    "    json.dump(labels_with_links_and_context_prompts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llama3_8b_context_prompts_results.json\", 'r') as f:\n",
    "    loaded_labels_with_links_and_context_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_prompt': '\\n\\nPretend that you are a classifier that predicts whether a post has civic content or not. Civic refers to whether a given post is related to politics (government, elections, politicians, activism, etc.) or social issues (major issues that affect a large group of people, such as the economy, inequality, racism, education, immigration, human rights, the environment, etc.). We refer to any content that is classified as being either of these two categories as “civic”; otherwise they are not civic. Please classify the following text denoted in <text> as \"civic\" or \"not civic\". \\n\\nThen, if the post is civic, classify the text based on the political lean of the opinion or argument it presents. Your options are \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', or \\'unclear\\'. You are analyzing text that has been pre-identified as \\'political\\' in nature. If the text is not civic, return \"unclear\".\\n\\nThink through your response step by step.\\n\\nReturn in a JSON format in the following way:\\n{\\n    \"civic\": <two values, \\'civic\\' or \\'not civic\\'>,\\n    \"political_ideology\": <four values, \\'left-leaning\\', \\'moderate\\', \\'right-leaning\\', \\'unclear\\'>,\\n    \"reason_civic\": <optional, a 1 sentence reason for why the text is civic>,\\n    \"reason_political_ideology\": <optional, a 1 sentence reason for why the text has the given political ideology>\\n}\\n\\n\\nHere is the post text that needs to be classified:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\n\\nThe classification of a post might depend on contextual information. For example, the text in a post might comment on an image or on a retweeted post. Attend to the context where appropriate. Here is some context on the post that needs classification: ```\\n<Content referenced or linked to in the post>\\n \\nThe post contains a external link to content with the following details:\\n```\\n[title]: Bernie Sanders Proposes Reducing Americans’ Workweek to 32 Hours\\n[description]: His proposal would pare down the workweek over a four-year period. The 40-hour workweek has stood as the standard in the U.S. since it became enshrined in federal law in 1940.\\n```\\n\\n<URLs>\\n The post links to external URLs:\\nThis post links to a trustworthy news article.\\n\\n\\n```\\nAgain, the text of the post that needs to be classified is:\\n```\\n<text>\\nthat’s right. there is a lot of very good evidence that americans are just as productive with a four-day work week and much happier to boot. what’s the point of having such a wealth society if we are not going to try to benefit from it?\\n```\\n\\nJustifications are not necessary.\\nReturn ONLY the JSON. I will parse the string result in JSON format.\\n',\n",
       " 'result': '{\\n    \"civic\": \"civic\",\\n    \"political_ideology\": \"left-leaning\",\\n    \"reason_civic\": \"The post references a specific policy proposal by Bernie Sanders, a left-leaning politician, and discusses the benefits of a four-day workweek.\",\\n    \"reason_political_ideology\": \"The post\\'s language and tone are consistent with left-leaning views on work-life balance and the role of government in promoting social welfare.\"\\n}'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_labels_with_links_and_context_prompts[example_link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's process our results and then compare it to our ground truth data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load our ground-truth labels, remove NAs, and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels: pd.DataFrame = pd.read_csv(\n",
    "    \"../manuscript_pilot/hand_labeled_pilot_posts.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels = ground_truth_labels[\n",
    "    ~pd.isna(ground_truth_labels[\"civic_hand_label\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels_subset = ground_truth_labels[\n",
    "    [\"link\", \"civic_hand_label\", \"political_ideology_hand_label\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process our labeled results and create a pandas dataframe. Let's transform our data into a list of dictionaries, and also process our JSON data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_context_prompted_processed_labels: list[dict] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (idx, (link, labeled_dict)) in enumerate(loaded_labels_with_links_and_context_prompts.items()):\n",
    "    res = {}\n",
    "    res[\"link\"] = link\n",
    "    res[\"prompt\"] = labeled_dict[\"context_prompt\"]\n",
    "    res[\"json_result\"] = labeled_dict[\"result\"]\n",
    "    result_dict = json.loads(labeled_dict[\"result\"])\n",
    "    res[\"valid_json_response\"] = True\n",
    "    res[\"hydrated_result\"] = result_dict\n",
    "    res[\"civic_label\"] = result_dict[\"civic\"]\n",
    "    res[\"political_label\"] = result_dict[\"political_ideology\"]\n",
    "    res[\"reason_civic_label\"] = result_dict.get(\"reason_civic\", None)\n",
    "    res[\"reason_political_label\"] = result_dict.get(\"reason_political_ideology\", None)\n",
    "    try:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error with post {post['link']} at index {idx}: {e}\")\n",
    "        res[\"valid_json_response\"] = False\n",
    "        res[\"hydrated_result\"] = None\n",
    "        res[\"civic_label\"] = None\n",
    "        res[\"political_label\"] = None\n",
    "        res[\"reason_civic_label\"] = None\n",
    "        res[\"reason_political_label\"] = None\n",
    "    finally:\n",
    "        llama3_8b_context_prompted_processed_labels.append(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check that we have as many processed JSONs as we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(llama3_8b_context_prompted_processed_labels))\n",
    "print(len(llama3_8b_context_prompted_processed_labels) == len(loaded_labels_with_links_and_context_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! This looks correct. Let's export these now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llama3_8b_context_prompts_processed_results.json\", 'w') as f:\n",
    "    json.dump(llama3_8b_context_prompted_processed_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pandas dataframe out of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_context_prompts_df = pd.DataFrame(llama3_8b_context_prompted_processed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>prompt</th>\n",
       "      <th>json_result</th>\n",
       "      <th>valid_json_response</th>\n",
       "      <th>hydrated_result</th>\n",
       "      <th>civic_label</th>\n",
       "      <th>political_label</th>\n",
       "      <th>reason_civic_label</th>\n",
       "      <th>reason_political_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://bsky.app/profile/jbouie.bsky.social/po...</td>\n",
       "      <td>\\n\\nPretend that you are a classifier that pre...</td>\n",
       "      <td>{\\n    \"civic\": \"civic\",\\n    \"political_ideol...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'civic': 'civic', 'political_ideology': 'left...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>The post references a specific policy proposal...</td>\n",
       "      <td>The post's language and tone are consistent wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://bsky.app/profile/lethalityjane.bsky.so...</td>\n",
       "      <td>\\n\\nPretend that you are a classifier that pre...</td>\n",
       "      <td>{\\n    \"civic\": \"civic\",\\n    \"political_ideol...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'civic': 'civic', 'political_ideology': 'uncl...</td>\n",
       "      <td>civic</td>\n",
       "      <td>unclear</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://bsky.app/profile/esqueer.bsky.social/p...</td>\n",
       "      <td>\\n\\nPretend that you are a classifier that pre...</td>\n",
       "      <td>{\\n    \"civic\": \"civic\",\\n    \"political_ideol...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'civic': 'civic', 'political_ideology': 'left...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>The post discusses censorship and neo-Nazism, ...</td>\n",
       "      <td>The post's language and tone suggest a left-le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://bsky.app/profile/bachynski.bsky.social...</td>\n",
       "      <td>\\n\\nPretend that you are a classifier that pre...</td>\n",
       "      <td>{\\n    \"civic\": \"civic\",\\n    \"political_ideol...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'civic': 'civic', 'political_ideology': 'left...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>The post references a historical event (Great ...</td>\n",
       "      <td>The post's appeal to the conscience of Irish A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://bsky.app/profile/rainsurname.bsky.soci...</td>\n",
       "      <td>\\n\\nPretend that you are a classifier that pre...</td>\n",
       "      <td>{\\n    \"civic\": \"civic\",\\n    \"political_ideol...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'civic': 'civic', 'political_ideology': 'left...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://bsky.app/profile/jbouie.bsky.social/po...   \n",
       "1  https://bsky.app/profile/lethalityjane.bsky.so...   \n",
       "2  https://bsky.app/profile/esqueer.bsky.social/p...   \n",
       "3  https://bsky.app/profile/bachynski.bsky.social...   \n",
       "4  https://bsky.app/profile/rainsurname.bsky.soci...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  \\n\\nPretend that you are a classifier that pre...   \n",
       "1  \\n\\nPretend that you are a classifier that pre...   \n",
       "2  \\n\\nPretend that you are a classifier that pre...   \n",
       "3  \\n\\nPretend that you are a classifier that pre...   \n",
       "4  \\n\\nPretend that you are a classifier that pre...   \n",
       "\n",
       "                                         json_result  valid_json_response  \\\n",
       "0  {\\n    \"civic\": \"civic\",\\n    \"political_ideol...                 True   \n",
       "1  {\\n    \"civic\": \"civic\",\\n    \"political_ideol...                 True   \n",
       "2  {\\n    \"civic\": \"civic\",\\n    \"political_ideol...                 True   \n",
       "3  {\\n    \"civic\": \"civic\",\\n    \"political_ideol...                 True   \n",
       "4  {\\n    \"civic\": \"civic\",\\n    \"political_ideol...                 True   \n",
       "\n",
       "                                     hydrated_result civic_label  \\\n",
       "0  {'civic': 'civic', 'political_ideology': 'left...       civic   \n",
       "1  {'civic': 'civic', 'political_ideology': 'uncl...       civic   \n",
       "2  {'civic': 'civic', 'political_ideology': 'left...       civic   \n",
       "3  {'civic': 'civic', 'political_ideology': 'left...       civic   \n",
       "4  {'civic': 'civic', 'political_ideology': 'left...       civic   \n",
       "\n",
       "  political_label                                 reason_civic_label  \\\n",
       "0    left-leaning  The post references a specific policy proposal...   \n",
       "1         unclear                                               None   \n",
       "2    left-leaning  The post discusses censorship and neo-Nazism, ...   \n",
       "3    left-leaning  The post references a historical event (Great ...   \n",
       "4    left-leaning                                               None   \n",
       "\n",
       "                              reason_political_label  \n",
       "0  The post's language and tone are consistent wi...  \n",
       "1                                               None  \n",
       "2  The post's language and tone suggest a left-le...  \n",
       "3  The post's appeal to the conscience of Irish A...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_8b_context_prompts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_context_prompts_df.to_csv(\"llama3_8b_context_prompts_processed_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! Let's compare against the ground truth labels and get some metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by getting some basic metrics of our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civic_label\n",
      "civic        141\n",
      "not civic     61\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(llama3_8b_context_prompts_df[\"civic_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political_label\n",
      "left-leaning     86\n",
      "unclear          35\n",
      "right-leaning    15\n",
      "moderate          5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    llama3_8b_context_prompts_df[\n",
    "        llama3_8b_context_prompts_df[\"civic_label\"] == \"civic\"\n",
    "    ][\"political_label\"].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare these to our ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_context_prompts_subset = (\n",
    "    llama3_8b_context_prompts_df[[\"link\", \"civic_label\", \"political_label\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>civic_hand_label</th>\n",
       "      <th>political_ideology_hand_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://bsky.app/profile/jbouie.bsky.social/po...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://bsky.app/profile/lethalityjane.bsky.so...</td>\n",
       "      <td>civic</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://bsky.app/profile/esqueer.bsky.social/p...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://bsky.app/profile/stuflemingnz.bsky.soc...</td>\n",
       "      <td>not civic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://bsky.app/profile/sararoseg.bsky.social...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link civic_hand_label  \\\n",
       "0  https://bsky.app/profile/jbouie.bsky.social/po...            civic   \n",
       "1  https://bsky.app/profile/lethalityjane.bsky.so...            civic   \n",
       "2  https://bsky.app/profile/esqueer.bsky.social/p...            civic   \n",
       "3  https://bsky.app/profile/stuflemingnz.bsky.soc...        not civic   \n",
       "4  https://bsky.app/profile/sararoseg.bsky.social...            civic   \n",
       "\n",
       "  political_ideology_hand_label  \n",
       "0                  left-leaning  \n",
       "1                       unclear  \n",
       "2                  left-leaning  \n",
       "3                           NaN  \n",
       "4                  left-leaning  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_labels_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_llama3_8b_context_results = pd.merge(\n",
    "    llama3_8b_context_prompts_subset, ground_truth_labels_subset,\n",
    "    on=\"link\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_llama3_8b_context_results.to_csv(\n",
    "    \"llama3_8b_context_results_joined_with_ground_truth.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>civic_label</th>\n",
       "      <th>political_label</th>\n",
       "      <th>civic_hand_label</th>\n",
       "      <th>political_ideology_hand_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://bsky.app/profile/jbouie.bsky.social/po...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://bsky.app/profile/lethalityjane.bsky.so...</td>\n",
       "      <td>civic</td>\n",
       "      <td>unclear</td>\n",
       "      <td>civic</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://bsky.app/profile/esqueer.bsky.social/p...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://bsky.app/profile/bachynski.bsky.social...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://bsky.app/profile/rainsurname.bsky.soci...</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "      <td>civic</td>\n",
       "      <td>left-leaning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link civic_label  \\\n",
       "0  https://bsky.app/profile/jbouie.bsky.social/po...       civic   \n",
       "1  https://bsky.app/profile/lethalityjane.bsky.so...       civic   \n",
       "2  https://bsky.app/profile/esqueer.bsky.social/p...       civic   \n",
       "3  https://bsky.app/profile/bachynski.bsky.social...       civic   \n",
       "4  https://bsky.app/profile/rainsurname.bsky.soci...       civic   \n",
       "\n",
       "  political_label civic_hand_label political_ideology_hand_label  \n",
       "0    left-leaning            civic                  left-leaning  \n",
       "1         unclear            civic                       unclear  \n",
       "2    left-leaning            civic                  left-leaning  \n",
       "3    left-leaning            civic                  left-leaning  \n",
       "4    left-leaning            civic                  left-leaning  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_llama3_8b_context_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 5)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_llama3_8b_context_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_context_civic_metrics = precision_recall_fscore_support(\n",
    "    y_true=joined_llama3_8b_context_results[\"civic_hand_label\"].tolist(),\n",
    "    y_pred=joined_llama3_8b_context_results[\"civic_label\"].tolist(),\n",
    "    average=\"binary\",\n",
    "    pos_label=\"civic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    llama3_8b_context_civic_precision,\n",
    "    llama3_8b_context_civic_recall,\n",
    "    llama3_8b_context_civic_fbeta_score,\n",
    "    llama3_8b_context_civic_support\n",
    ") = llama3_8b_context_civic_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_context_total_correct = sum([\n",
    "    pred == true_label\n",
    "    for (pred, true_label)\n",
    "    in zip(\n",
    "        joined_llama3_8b_context_results[\"civic_label\"].tolist(),\n",
    "        joined_llama3_8b_context_results[\"civic_hand_label\"].tolist()\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b_context_accuracy = (\n",
    "    llama3_8b_context_total_correct / len(joined_llama3_8b_context_results)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8267326732673267\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {llama3_8b_context_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8297872340425532\tRecall: 0.9140625\tF-1 score: 0.8698884758364313\tSupport: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {llama3_8b_context_civic_precision}\\tRecall: {llama3_8b_context_civic_recall}\\tF-1 score: {llama3_8b_context_civic_fbeta_score}\\tSupport: {llama3_8b_context_civic_support}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's calculate for political ideology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run the code now for the non-context prompts.\n",
    "\n",
    "Let's now run the same thing but for the non-context prompts.\n",
    "\n",
    "We'll then have to do two analyses:\n",
    "- Labeled performance of all non-context prompts vs. ground truth posts\n",
    "- Labeled performance of posts that have context and non-context prompts, and see how they compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_labels_with_links_and_context_prompts[example_link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bluesky-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
