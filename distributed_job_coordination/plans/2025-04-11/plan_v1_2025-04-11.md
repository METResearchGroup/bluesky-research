# Project Requirements Document (PRD)

## Title: Serverless Distributed Job Orchestration Framework for Slurm + AWS

---

## 1. Executive Summary

This PRD outlines the design, requirements, and high-level architecture of a robust, serverless distributed job orchestration framework designed for HPC environments (e.g., Slurm) and cloud-native coordination (e.g., AWS S3, DynamoDB). The system enables scalable, fault-tolerant, and reusable compute workflows for embarrassingly parallel tasks—ideal for research labs and large-scale data processing projects.

---

## 2. Motivation

### Current Challenges

- Long-running jobs on Slurm are fragile and non-resumable.
- There is no unified system for distributed coordination across hundreds of thousands of tasks.
- Most academic pipelines lack durability, traceability, and retry capabilities.
- Scaling beyond a few thousand users is prohibitively slow without massive manual coordination.

### Why This Project Matters

- Empowers research teams with a portable, reproducible compute system.
- Minimizes operational burden by removing the need for persistent infrastructure.
- Enables large-scale jobs (e.g., 5M+ users) to run with rate-limited APIs in fault-tolerant ways.
- Makes computation pipelines reusable, parameterizable, and easy to extend.

---

## 3. Goals and Non-Goals

### Goals:
- Implement a robust coordinator-worker architecture with Slurm submission and AWS coordination.
- Support plugin-style job definitions with reusable handler functions.
- Ensure rate limit compliance, checkpointing, retries, and final aggregation.
- Deliver structured logging, status tracking, and dashboards.
- Make the system auditable and resumable at any scale.

### Non-Goals

- Real-time, latency-sensitive job execution (focus is batch processing).
- Supporting multi-cluster or cloud-native job execution (Slurm first).
- Full multi-user auth/ACL support (can be added later).

---

## 4. System Overview

### Core Components

- **Coordinator**: Parses inputs, generates task definitions, submits Slurm jobs.
- **Worker**: Slurm-executed job that pulls tasks, executes handler, and logs results.
- **DynamoDB**: Shared state store for progress, rate limits, and checkpoints.
- **S3**: Durable storage for input payloads, intermediate results, and final Parquet datasets.
- **Aggregator**: Merges SQLite or JSONL outputs into a unified Parquet dataset.
- **Retry Handler**: Monitors failed tasks and selectively resubmits based on failure type.

---

## 5. Design Principles

- **Batch-oriented**: Optimize for throughput over latency.
- **Fault-tolerant**: No single point of failure.
- **Stateless workers**: All state externalized to S3/Dynamo.
- **Composable**: Jobs are handlers + config, no internal state dependencies.
- **Portable**: Runs in any Slurm-based compute environment.
- **Observable**: Progress, errors, and retries are visible and auditable.

---

## 6. Scalability Strategy

### Scaling Factors

- Number of users processed per job.
- Number of Slurm jobs submitted.
- DynamoDB write volume.
- S3 PUT/GET frequency.

### Mitigations

- Use batch task bundling (e.g., 100–1000 users per task).
- Implement pre-token leasing for rate limits.
- Store intermediate writes in SQLite with resumable aggregation logic.
- Use prefix sharding and caching for DynamoDB access.

---

## 7. Fault Tolerance

### Anticipated Failures

- Coordinator job crash → restartable via checkpoints.
- Slurm job preemption → resumable from last checkpoint.
- S3 write failure → retried with `.done` marker-based confirmation.
- Rate limit spikes → handled via backoff and token lease coordination.

### Recovery Plan

- Task retries tracked in Dynamo.
- Result batches re-uploaded only if missing `.done` file.
- Dashboard alerts triggered via anomaly detection in logs/errors.

---

## 8. Interfaces

### CLI

```bash
slurmctl submit jobs/backfill/config.yaml
slurmctl status backfill-2025-04-11
slurmctl retry --job-id backfill-2025-04-11 --errors=network
```
