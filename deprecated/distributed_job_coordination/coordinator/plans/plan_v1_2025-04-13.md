# Coordinator Implementation Plan (v1 2025-04-13)

## Overview

The Coordinator is the central orchestration component of our Serverless Distributed Job Orchestration Framework. Based on the architecture diagram and requirements in the PRD, the Coordinator is responsible for:

1. Parsing and validating job configurations
2. Partitioning input data into batches
3. Writing batch data to S3 and DynamoDB
4. Submitting Slurm jobs to process batches
5. Initializing and tracking job state

This document outlines a step-by-step implementation plan for the Coordinator module, referencing the existing infrastructure diagram and utilizing the `create_batches` function from `lib/helper.py`.

## Architecture Context

According to the diagram in `diagram_v1.md`, the Coordinator has several key responsibilities:

```
subgraph Coordinator
    direction TB
    A[Split records into batches]
    A2[Writer]
    B[Kick off workers]
    F[Write read-only copy of batches]
    I[Write new write-friendly batches]
    J[Update job status to 'start']
end
```

The Coordinator interacts with AWS services (S3, DynamoDB, SQS) and initiates the Worker processes through Slurm job submission. This plan will implement these components in a modular, extensible way.

## Implementation Plan

### Phase 1: Core Abstractions and Models

1. **Job Configuration Handling**
   - Create `JobConfig` class with YAML parsing and validation
   - Implement configuration schema with defaults
   - Add validation rules for required fields
   - Support for inheritance and overrides

2. **Job State Models**
   - Implement `Job` class representing overall job
   - Create `Task` and `TaskGroup` abstractions
   - Implement `Batch` class for data partitioning
   - Define serialization/deserialization methods

3. **Manifest System**
   - Implement `JobManifest` class for job metadata
   - Create `TaskManifest` for task-specific information
   - Define manifest storage and retrieval utilities
   - Add checksum validation for manifests

### Phase 2: Core Coordinator Functionality

1. **Data Partitioning**
   - Utilize existing `create_batches` function from `lib/helper.py`
   - Add support for different input formats (JSONL, CSV, Parquet)
   - Implement efficient batch generation for large inputs
   - Create batch metadata with tracking information

2. **Storage Integration**
   - Implement S3 storage for batch data and manifests
   - Create DynamoDB integration for job and task status
   - Add atomic operations for state transitions
   - Implement manifest versioning

3. **Job Dispatcher**
   - Create Slurm job submission interface
   - Support job arrays for worker tasks
   - Implement resource allocation logic
   - Add job ID tracking and correlation

### Phase 3: Reliability Features

1. **State Management**
   - Implement optimistic locking for state updates
   - Add recovery mechanisms for coordinator failures
   - Create utilities for state queries
   - Implement job status transitions

2. **Error Handling**
   - Add robust error handling for all operations
   - Implement clean failure paths
   - Create detailed error logging
   - Support for manual intervention

3. **Testing Strategy**
   - Unit tests for all coordinator components
   - Integration tests for AWS service interactions
   - Mock Slurm interface for testing
   - End-to-end coordinator workflow tests

## Component Design

### JobConfig Class

```python
class JobConfig:
    """Configuration for a distributed job."""
    
    def __init__(self, config_path: str):
        """Load configuration from YAML file."""
        
    def validate(self) -> bool:
        """Validate configuration against schema."""
        
    @property
    def handler_path(self) -> str:
        """Get the handler function path."""
        
    @property
    def batch_size(self) -> int:
        """Get the batch size for this job."""
        
    # Additional configuration properties
```

### Job Class

```python
class Job:
    """Represents a distributed job execution."""
    
    def __init__(self, job_id: str, config: JobConfig):
        """Initialize a new job with given ID and configuration."""
        self.job_id = job_id
        self.config = config
        self.status = JobStatus.PENDING
        self.tasks = []
        
    def generate_manifest(self) -> dict:
        """Generate job manifest as dictionary."""
        
    def save_manifest(self, s3_client) -> str:
        """Save job manifest to S3 and return path."""
        
    # Additional job methods
```

### DataPartitioner Class

```python
class DataPartitioner:
    """Handles partitioning input data into batches."""
    
    def __init__(self, input_path: str, batch_size: int):
        """Initialize partitioner with input file and batch size."""
        
    def read_input_data(self) -> list:
        """Read and parse input data file."""
        
    def create_batches(self) -> list:
        """Create batches using the helper function."""
        from lib.helper import create_batches
        return create_batches(self.data, self.batch_size)
        
    def generate_batch_manifests(self, batches: list) -> list:
        """Generate manifest for each batch."""
        
    # Additional partitioner methods
```

### Coordinator Class

```python
class Coordinator:
    """Main coordinator for distributed job orchestration."""
    
    def __init__(self, config_path: str, input_path: str = None):
        """Initialize coordinator with job configuration file."""
        
    def validate_config(self) -> bool:
        """Validate job configuration."""
        
    def prepare_job(self) -> str:
        """Prepare job by creating ID, manifests, etc."""
        
    def partition_data(self) -> list:
        """Partition input data into batches."""
        
    def store_batches(self, batches: list) -> None:
        """Store batches in S3 and DynamoDB."""
        
    def submit_job(self) -> str:
        """Submit Slurm job array for processing."""
        
    def update_job_status(self, status: str) -> None:
        """Update job status in DynamoDB and manifest."""
        
    # Additional coordinator methods
```

## Implementation Timeline

| Component | Estimated Effort | Dependencies |
|-----------|------------------|--------------|
| Job Configuration | 2-3 days | None |
| Data Models | 3-4 days | Job Configuration |
| Manifest System | 2-3 days | Data Models |
| Data Partitioning | 2-3 days | `create_batches` function |
| Storage Integration | 3-4 days | Manifest System, Data Partitioning |
| Job Dispatcher | 2-3 days | Storage Integration |
| State Management | 3-4 days | Job Dispatcher |
| Error Handling | 2-3 days | All components |
| Tests | Throughout | Component being tested |

## Implementation Steps

### Step 1: Create JobConfig Class
1. Implement YAML parsing with error handling
2. Create schema validation
3. Add default values for optional parameters
4. Implement configuration inheritance

### Step 2: Implement Job and Task Models
1. Create Job class with state management
2. Implement Task and TaskGroup abstractions
3. Add Batch representation for data partitioning
4. Implement serialization/deserialization

### Step 3: Build Manifest System
1. Create JobManifest class
2. Implement TaskManifest for task tracking
3. Add S3 storage utilities for manifests
4. Implement checksum validation

### Step 4: Implement Data Partitioning
1. Integrate with existing `create_batches` function
2. Add support for different input formats
3. Implement batch metadata generation
4. Create batch storage utilities

### Step 5: Add AWS Integration
1. Implement S3 client for data storage
2. Add DynamoDB integration for state tracking
3. Create atomic operations for updates
4. Implement versioning and conflict resolution

### Step 6: Create Job Submission Interface
1. Implement Slurm job submission
2. Add support for job arrays
3. Create resource calculation logic
4. Implement job ID tracking

### Step 7: Implement State Management
1. Add state transition validation
2. Implement optimistic locking
3. Create recovery mechanisms
4. Add job status queries

### Step 8: Comprehensive Error Handling
1. Implement exception handling hierarchy
2. Add retry logic for transient failures
3. Create detailed error logging
4. Add manual intervention support

### Step 9: Testing
1. Write unit tests for all components
2. Implement integration tests for AWS interactions
3. Create mock interfaces for testing
4. Add end-to-end workflow tests

## Interfaces with Other Components

### Worker Interface
- Coordinator generates task definitions that workers consume
- Task manifests include all information needed for worker execution
- Workers update task status directly in DynamoDB

### Aggregator Interface
- Coordinator initiates aggregation phase after worker completion
- Aggregator reads results based on job manifests
- Final job status updated after aggregation completes

### AWS Integration
- Uses S3 for manifest and batch data storage
- Uses DynamoDB for job and task status tracking
- Supports SQS for retry coordination (future phase)

## Next Steps After Coordinator Implementation

1. Worker implementation with local storage management
2. Result publishing from workers to S3
3. Aggregation system for combining results
4. Enhanced state management with retries
5. CLI for job control and monitoring 