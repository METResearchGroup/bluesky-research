# Project Requirements Document (PRD): v2

## Title: Serverless Distributed Job Orchestration Framework for Slurm + AWS

*Revised PRD generated after reviewing plan_v1_2025-04-11.md and revisions_v1_2025-04-11.md*

---

## 1. Executive Summary

This PRD outlines a robust, serverless distributed job orchestration framework designed for HPC environments (Slurm) with cloud-native coordination (AWS S3, DynamoDB, SQS). The system enables scalable, fault-tolerant, and reusable compute workflows for embarrassingly parallel tasks—ideal for research labs processing millions of items with rate-limited APIs and large-scale data processing pipelines.

---

## 2. Motivation

### Current Challenges

- Long-running jobs on Slurm are fragile and non-resumable
- Lack of unified system for coordinating hundreds of thousands of distributed tasks
- Academic pipelines typically lack durability, traceability, and retry capabilities
- Current approaches fail to scale beyond a few thousand users without massive manual coordination

### Why This Project Matters

- Provides portable, reproducible compute infrastructure for research teams
- Minimizes operational burden through serverless coordination with no persistent infrastructure
- Enables fault-tolerant processing of large-scale jobs (5M+ users) against rate-limited APIs
- Makes computation pipelines reusable, parameterizable, and extensible

---

## 3. Goals and Non-Goals

### Goals

- Implement a robust coordinator-worker architecture using Slurm for execution and AWS for coordination
- Support plugin-style job definitions with reusable handler functions
- Ensure rate limit compliance, checkpointing, intelligent retries, and reliable aggregation
- Deliver structured logging, status tracking, and operational dashboards
- Make the system auditable and resumable at any scale

### Non-Goals

- Real-time, latency-sensitive job execution (batch processing is the focus)
- Multi-cluster or pure cloud-native job execution (Slurm is the primary compute platform)
- Full multi-user auth/ACL support (single-user/team focus initially)

---

## 4. System Architecture

### Core Components

![Architecture Diagram](revisions_v1_2025-04-11.md#mermaid-diagram-of-proposed-architecture)

#### Coordinator
- Generates task definitions and batches input data
- Submits Slurm jobs and initializes job state
- Writes manifests and batch data to S3/DynamoDB
- Tracks overall job progress and status

#### Workers
- Execute as Slurm jobs, processing assigned batch(es)
- Read batch data from S3, update progress in DynamoDB
- Store intermediate results in SQLite on local scratch storage
- Write successful results to S3 with completion markers

#### Aggregator
- Merges SQLite or Parquet outputs into unified datasets
- Supports hierarchical aggregation for very large jobs
- Validates data integrity before finalizing
- Generates final job completion manifests

#### State Management
- DynamoDB: Job metadata, task status, rate limit tokens
- S3: Input/output data, manifests, checkpoints, logs
- SQS: Failed task queuing and retry coordination

---

## 5. Core Abstractions

### Job
- One submission of a complete distributed workflow
- Has unique `job_id` (e.g., `backfill-2025-04-12`)
- Defined by a `JobDefinition` config template
- Execution instance tracked as a `JobRun`

### Task
- Unit of execution within a job
- Types include:
  - **Worker tasks**: Process data batches
  - **System tasks**: Coordinator, Aggregator, Retry Planner

### Task Metadata
```json
{
  "task_id": "task-0042",
  "job_id": "backfill-2025-04-12",
  "phase": "phase_1_backfill",
  "group": "initial_batch", 
  "role": "worker",
  "batch_id": "batch-0042",
  "status": "SUCCESS",
  "retries": 0
}
```

### Batch
- Data partition to be processed (e.g., 100 users)
- Separate from task execution for easy retry/reprocessing
- Stored in S3 in read-only form and DynamoDB for status updates

### TaskGroup/Phase
- **TaskGroup**: Tasks with same role and configuration
  - Examples: `initial_batch`, `retry_batch_1`, `final_aggregation`
- **Phase**: High-level sequential portion of a job
  - Examples: `phase_1_backfill`, `phase_2_retry`, `phase_3_aggregate`

---

## 6. Manifest System

### Job Manifest
```json
{
  "job_id": "backfill-2025-04-12",
  "job_type": "backfill",
  "handler": "jobs.backfill.handler",
  "git_commit": "a3c123f",
  "config_file": "jobs/backfill/config.yaml",
  "input_file": "s3://bucket/input/users-2025-04-12.jsonl",
  "batch_size": 100,
  "task_count": 10000,
  "slurm_array_id": "123456",
  "submitted_at": "2025-04-12T12:00:00Z",
  "phases": ["initial", "retry", "aggregation"],
  "status": "RUNNING"
}
```

### Task Manifest
```json
{
  "job_id": "backfill-2025-04-12",
  "task_id": "task-0042",
  "batch_id": "batch-0042",
  "role": "worker",
  "phase": "initial",
  "group": "initial_batch",
  "input_s3": "s3://bucket/jobs/backfill-2025-04-12/batches/batch-0042.json",
  "output_file": "/scratch/job-2025-04-12/task-0042.sqlite",
  "status": "SUCCESS",
  "start_time": "2025-04-12T12:05:00Z",
  "end_time": "2025-04-12T12:07:22Z",
  "metrics": {
    "users_processed": 100,
    "api_requests": 153
  },
  "error": null
}
```

### Result & Retry Manifests
- Result manifest includes file path, status, validation checksums
- Retry manifest tracks failed tasks, error types, retry attempts
- All manifests stored in S3 for durability and auditability

---

## 7. Workflow Execution

### Job Submission
1. User invokes CLI: `slurmctl submit jobs/backfill/config.yaml`
2. Coordinator parses config, validates input data
3. Coordinator generates batches, writes to S3 and DynamoDB
4. Job manifest written to S3

### Worker Execution
1. Worker task starts via Slurm job array
2. Reads batch data from S3
3. Processes items using job-specific handler
4. Stores results in local SQLite and updates progress in DynamoDB
5. On completion, uploads results to S3 with `.done` marker

### Retry Mechanism
1. Retry planner scans task manifests for failures
2. Categorizes failures (transient vs. permanent)
3. Queues retriable failures to SQS
4. New retry tasks created with appropriate backoff
5. Permanent failures logged for analysis

### Result Aggregation
1. Triggered after worker completion or by threshold
2. For large jobs, uses hierarchical aggregation:
   - Level 1: Merge 10 task outputs → intermediate files
   - Level 2: Merge intermediate files → final output
3. Validates output with checksums
4. Updates job status to COMPLETED

---

## 8. Storage Strategy

### Local Storage
- Worker tasks use `/scratch/job_<job_id>/task_<task_id>/` for local processing
- SQLite databases for intermediate data processing
- Local Parquet for efficient task output

### Cloud Storage
- S3: Durable storage for input/output data, manifests, logs
- DynamoDB: Task status, progress tracking, rate limit tokens
- SQS: Failure queuing, retry coordination

---

## 9. Rate Limiting

### Token Management
- DynamoDB-based distributed rate limiter
- Coordinator allocates token buckets to workers
- Workers self-regulate based on token availability
- Automatic backoff during API throttling

### Implementation
```python
class DistributedRateLimiter:
    def __init__(self, job_id, tokens_per_sec, dynamo_client):
        self.job_id = job_id
        self.tokens_per_sec = tokens_per_sec
        self.dynamo = dynamo_client
        
    def acquire_token_lease(self, worker_id, count=10):
        # Atomically request tokens from shared pool
        
    def release_unused_tokens(self, count):
        # Return unused tokens to pool
```

---

## 10. Fault Tolerance

### Coordinator Failures
- Job state persisted in S3/DynamoDB
- Restartable from last checkpoint
- Can be manually restarted: `slurmctl resume <job_id>`

### Worker Failures
- Slurm preemption handled via task manifests
- Network failures automatically retried with backoff
- Permanent failures (e.g., deleted resources) logged and skipped

### Aggregation Failures
- Atomic file operations with validation
- `.done` marker system prevents partial aggregation
- Restartable at any point in hierarchy

---

## 11. Observability

### Metrics Collection
- Per-task resource usage (CPU, memory, network)
- API call counts and latencies
- Batch processing times
- Retry counts by error type

### Operational Dashboard
- Overall job progress (tasks complete/failed)
- Processing rate (items/second)
- Error distribution and hotspots
- Resource utilization

### Alerting
- Stalled job detection
- Abnormal error rate notification
- Rate limit exhaustion warnings
- Infrastructure failure escalation

---

## 12. CLI Interface

### Job Management
```bash
# Submit new job
slurmctl submit jobs/backfill/config.yaml

# Check job status
slurmctl status backfill-2025-04-11

# Retry specific failed tasks
slurmctl retry --job-id backfill-2025-04-11 --errors=network
```

### Operational Commands
```bash
# Pause job (suspend token issuance)
slurmctl pause backfill-2025-04-11

# Resume paused job
slurmctl resume backfill-2025-04-11

# Analyze error patterns
slurmctl analyze-errors backfill-2025-04-11
```

---

## 13. Implementation Plan

### Phase 1: Core Framework
- Coordinator implementation with batch generation
- Basic worker task execution with DynamoDB state
- S3 integration for manifest storage
- Initial CLI commands for job submission

### Phase 2: Reliability Features
- Retry mechanism with error classification
- Hierarchical aggregation implementation
- Token-based rate limiting
- Task progress tracking and resumability

### Phase 3: Operations & Monitoring
- Dashboard for job progress and errors
- Alert integration for failure notification
- Advanced CLI for job management
- Documentation and example job handlers

---

## 14. Testing Strategy

### Unit Testing
- Handler function testing with mock data
- Rate limiter behavior verification
- Manifest parsing/generation validation

### Integration Testing
- End-to-end small-scale job execution
- Network failure simulation
- API throttling simulation

### Scale Testing
- Synthetic load testing with 10k+ tasks
- DynamoDB throughput validation
- S3 concurrency testing

---

## 15. Security Considerations

### Data Protection
- S3 bucket policies for access control
- IAM roles with least privilege access
- No sensitive data in logs or manifests

### Credentials Management
- AWS credential rotation support
- Support for IAM roles for tasks
- No hardcoded secrets in configuration

---

## 16. Resource Requirements

### AWS Resources
- DynamoDB: On-demand capacity for job metadata
- S3: Standard storage for manifests, data
- SQS: Standard queues for retry coordination

### Slurm Resources
- Per-worker: 2-4 CPU cores, 8-16GB RAM
- Local storage: ~10GB per task in `/scratch`
- Total concurrent tasks: Depends on cluster capacity 